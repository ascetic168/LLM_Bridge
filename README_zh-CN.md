# OpenAI å…¼å®¹ MCP å®¢æˆ·ç«¯

ğŸŒ **è¯­è¨€**: [English](README.md) | [ç¹é«”ä¸­æ–‡](README_zh-TW.md) | [ç®€ä½“ä¸­æ–‡](README_zh-CN.md)

ä¸€ä¸ªç”¨äºè¿æ¥ OpenAI å…¼å®¹ LLM æ¥æºçš„ MCP (Model Context Protocol) å®¢æˆ·ç«¯ï¼Œè®©æ‚¨åœ¨ Claude Code ä¸­å¯ä»¥éšæ—¶è°ƒç”¨ä¸åŒçš„ LLM æœåŠ¡ã€‚

## åŠŸèƒ½ç‰¹è‰²

- âœ… æ”¯æŒæ‰€æœ‰ OpenAI API å…¼å®¹çš„æœåŠ¡
- âœ… å¤šé…ç½®ç®¡ç†ï¼Œå¯åŒæ—¶è¿æ¥å¤šä¸ª LLM æ¥æº
- âœ… ç¯å¢ƒå˜é‡é…ç½®ï¼Œæ–¹ä¾¿éƒ¨ç½²
- âœ… TypeScript å¼€å‘ï¼Œç±»å‹å®‰å…¨

## å®‰è£…

```bash
npm install
npm run build

# å¤åˆ¶ç¯å¢ƒæ–‡ä»¶å¹¶ç¼–è¾‘æ‚¨çš„é…ç½®
cp .env.example .env
# ç„¶åç¼–è¾‘ .env æ–‡ä»¶ï¼Œå¡«å…¥æ‚¨çš„ API å¯†é’¥å’Œè®¾ç½®
```

## é…ç½®

### ç¯å¢ƒå˜é‡

```bash
# é»˜è®¤é…ç½®
OPENAI_BASE_URL=https://api.openai.com/v1
OPENAI_API_KEY=your_api_key_here
OPENAI_MODEL=gpt-3.5-turbo
OPENAI_TEMPERATURE=0.7
OPENAI_MAX_TOKENS=1000

# å¤šé…ç½®æ”¯æŒ (JSON æ ¼å¼)
LLM_CONFIGS='[
  {
    "name": "openai",
    "baseUrl": "https://api.openai.com/v1",
    "apiKey": "sk-...",
    "model": "gpt-4"
  },
  {
    "name": "local",
    "baseUrl": "http://localhost:8080/v1",
    "model": "qwen"
  }
]'
```

### æ”¯æŒçš„æœåŠ¡

- OpenAI API
- Azure OpenAI
- Local AI (Ollama, vLLM, Text Generation Inference)
- ä»»ä½• OpenAI å…¼å®¹çš„ API ç«¯ç‚¹

## ä½¿ç”¨æ–¹æ³•

### 1. å¯åŠ¨ MCP æœåŠ¡å™¨

```bash
npm start
```

### 2. åœ¨ Claude Code ä¸­é…ç½®

#### é€‰é¡¹ 1: æ‰‹åŠ¨é…ç½®
åœ¨ Claude Code çš„è®¾ç½®ä¸­åŠ å…¥ï¼š

```json
{
  "mcpServers": {
    "openai-client": {
      "command": "node",
      "args": ["/path/to/your/mcp-openai-client/dist/index.js"],
      "env": {
        "OPENAI_API_KEY": "your_api_key_here"
      }
    }
  }
}
```

#### é€‰é¡¹ 2: ä½¿ç”¨ Claude CLI
æ‚¨ä¹Ÿå¯ä»¥ä½¿ç”¨ Claude CLI å‘½ä»¤æ¥æ·»åŠ  MCP æœåŠ¡å™¨ï¼š

```bash
claude mcp add openai-client node /path/to/your/mcp-openai-client/dist/index.js
```

å°† `/path/to/your/mcp-openai-client/dist/index.js` æ›¿æ¢ä¸ºæ‚¨ç¼–è¯‘åçš„ index.js æ–‡ä»¶çš„å®é™…è·¯å¾„ã€‚

### 3. åœ¨ Claude Code ä¸­è°ƒç”¨ LLM

åœ¨ Claude Code ä¸­ä½¿ç”¨è‡ªç„¶è¯­è¨€æç¤ºè¯æ¥å¼•ç”¨æ‚¨é…ç½®çš„ LLMï¼š

```
'qwen' æ˜¯é…ç½®çš„ MCP openai-client çš„ Qwen æ¨¡å‹è¿æ¥ã€‚è¯·ä½¿ç”¨ 'qwen' æ¥å¸®æˆ‘åˆ†æè¿™ä»½æŠ€æœ¯æ–‡æ¡£å¹¶æä¾›æ‘˜è¦ã€‚
```

```
ä½¿ç”¨ qwenï¼Œè¯·å¸®æˆ‘ç”Ÿæˆæ•°æ®å¤„ç†çš„ Python ä»£ç ã€‚
```

```
è¯·ä½¿ç”¨ MCP openai-client çš„é»˜è®¤é…ç½®æ¥å¸®æˆ‘å°†è¿™æ®µæ–‡å­—ç¿»è¯‘æˆæ³•æ–‡ã€‚OpenAI æ¨¡å‹åº”è¯¥èƒ½å¾ˆå¥½åœ°å¤„ç†ç¿»è¯‘ä»»åŠ¡ã€‚
```

è¿›é˜¶ä½¿ç”¨æ—¶ï¼Œæ‚¨ä¹Ÿå¯ä»¥ç›´æ¥ä½¿ç”¨å·¥å…·ï¼š

```javascript
// è°ƒç”¨ç‰¹å®šé…ç½® (Qwen æ¨¡å‹)
const result = await mcp.callTool('llm-complete', {
  prompt: 'ç¼–å†™å¤„ç† CSV æ•°æ®çš„ Python å‡½æ•°',
  configName: 'local',  // ä½¿ç”¨ Qwen é…ç½®
  maxTokens: 1000
});

// åˆ—å‡ºå¯ç”¨é…ç½®
const configs = await mcp.callTool('llm-list-configs', {});
```

## å·¥å…·æ–¹æ³•

### `llm-complete`

å‘é€æç¤ºè¯åˆ° LLM å¹¶è·å–å›åº”ã€‚

**å‚æ•°ï¼š**
- `prompt` (string): è¦å‘é€çš„æç¤ºè¯
- `configName` (string, optional): é…ç½®åç§°ï¼Œé»˜è®¤ä¸º 'default'
- `temperature` (number, optional): æ¸©åº¦å‚æ•° (0.0 åˆ° 2.0)
- `maxTokens` (number, optional): æœ€å¤§ token æ•°

**è¿”å›ï¼š**
```typescript
{
  content: [{
    type: 'text',
    text: string      // LLM å›åº”å†…å®¹
  }],
  isError: boolean    // æ˜¯å¦å‘ç”Ÿé”™è¯¯
}
```

### `llm-list-configs`

åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„ LLM é…ç½®ã€‚

**è¿”å›ï¼š**
```typescript
{
  content: [{
    type: 'text',
    text: string      // JSON æ ¼å¼çš„é…ç½®åˆ—è¡¨
  }],
  isError: boolean    // æ˜¯å¦å‘ç”Ÿé”™è¯¯
}
```

## å¼€å‘

```bash
# å¼€å‘æ¨¡å¼ (ç›‘å¬æ–‡ä»¶å˜åŒ–)
npm run dev

# ç¼–è¯‘ TypeScript
npm run build

# æ‰§è¡Œç¼–è¯‘åçš„ç¨‹åº
npm run start
```

## æˆæƒ

MIT License