# OpenAI ç›¸å®¹ MCP å®¢æˆ¶ç«¯

ğŸŒ **èªè¨€**: [English](README.md) | [ç¹é«”ä¸­æ–‡](README_zh-TW.md) | [ç®€ä½“ä¸­æ–‡](README_zh-CN.md)

ä¸€å€‹ç”¨æ–¼é€£æ¥ OpenAI ç›¸å®¹ LLM ä¾†æºçš„ MCP (Model Context Protocol) å®¢æˆ¶ç«¯ï¼Œè®“æ‚¨åœ¨ Claude Code ä¸­å¯ä»¥éš¨æ™‚èª¿ç”¨ä¸åŒçš„ LLM æœå‹™ã€‚

## åŠŸèƒ½ç‰¹è‰²

- âœ… æ”¯æ´æ‰€æœ‰ OpenAI API ç›¸å®¹çš„æœå‹™
- âœ… å¤šé…ç½®ç®¡ç†ï¼Œå¯åŒæ™‚é€£æ¥å¤šå€‹ LLM ä¾†æº
- âœ… ç’°å¢ƒè®Šæ•¸é…ç½®ï¼Œæ–¹ä¾¿éƒ¨ç½²
- âœ… TypeScript é–‹ç™¼ï¼Œå‹åˆ¥å®‰å…¨

## å®‰è£

```bash
npm install
npm run build

# è¤‡è£½ç’°å¢ƒæª”æ¡ˆä¸¦ç·¨è¼¯æ‚¨çš„é…ç½®
cp .env.example .env
# ç„¶å¾Œç·¨è¼¯ .env æª”æ¡ˆï¼Œå¡«å…¥æ‚¨çš„ API é‡‘é‘°å’Œè¨­å®š
```

## é…ç½®

### ç’°å¢ƒè®Šæ•¸

```bash
# é è¨­é…ç½®
OPENAI_BASE_URL=https://api.openai.com/v1
OPENAI_API_KEY=your_api_key_here
OPENAI_MODEL=gpt-3.5-turbo
OPENAI_TEMPERATURE=0.7
OPENAI_MAX_TOKENS=1000

# å¤šé…ç½®æ”¯æ´ (JSON æ ¼å¼)
LLM_CONFIGS='[
  {
    "name": "openai",
    "baseUrl": "https://api.openai.com/v1",
    "apiKey": "sk-...",
    "model": "gpt-4"
  },
  {
    "name": "local",
    "baseUrl": "http://localhost:8080/v1",
    "model": "qwen"
  }
]'
```

### æ”¯æ´çš„æœå‹™

- OpenAI API
- Azure OpenAI
- Local AI (Ollama, vLLM, Text Generation Inference)
- ä»»ä½• OpenAI ç›¸å®¹çš„ API ç«¯é»

## ä½¿ç”¨æ–¹æ³•

### 1. å•Ÿå‹• MCP ä¼ºæœå™¨

```bash
npm start
```

### 2. åœ¨ Claude Code ä¸­é…ç½®

#### é¸é … 1: æ‰‹å‹•é…ç½®
åœ¨ Claude Code çš„è¨­å®šä¸­åŠ å…¥ï¼š

```json
{
  "mcpServers": {
    "openai-client": {
      "command": "node",
      "args": ["/path/to/your/mcp-openai-client/dist/index.js"],
      "env": {
        "OPENAI_API_KEY": "your_api_key_here"
      }
    }
  }
}
```

#### é¸é … 2: ä½¿ç”¨ Claude CLI
æ‚¨ä¹Ÿå¯ä»¥ä½¿ç”¨ Claude CLI å‘½ä»¤ä¾†æ–°å¢ MCP ä¼ºæœå™¨ï¼š

```bash
claude mcp add openai-client node /path/to/your/mcp-openai-client/dist/index.js
```

å°‡ `/path/to/your/mcp-openai-client/dist/index.js` æ›¿æ›ç‚ºæ‚¨ç·¨è­¯å¾Œçš„ index.js æª”æ¡ˆçš„å¯¦éš›è·¯å¾‘ã€‚

### 3. åœ¨ Claude Code ä¸­å‘¼å« LLM

åœ¨ Claude Code ä¸­ä½¿ç”¨è‡ªç„¶èªè¨€æç¤ºè©ä¾†å¼•ç”¨æ‚¨é…ç½®çš„ LLMï¼š

```
'qwen' æ˜¯é…ç½®çš„ MCP openai-client çš„ Qwen æ¨¡å‹é€£æ¥ã€‚è«‹ä½¿ç”¨ 'qwen' ä¾†å¹«æˆ‘åˆ†æé€™ä»½æŠ€è¡“æ–‡ä»¶ä¸¦æä¾›æ‘˜è¦ã€‚
```

```
ä½¿ç”¨ qwenï¼Œè«‹å¹«æˆ‘ç”Ÿæˆè³‡æ–™è™•ç†çš„ Python ç¨‹å¼ç¢¼ã€‚
```

```
è«‹ä½¿ç”¨ MCP openai-client çš„é è¨­é…ç½®ä¾†å¹«æˆ‘å°‡é€™æ®µæ–‡å­—ç¿»è­¯æˆæ³•æ–‡ã€‚OpenAI æ¨¡å‹æ‡‰è©²èƒ½å¾ˆå¥½åœ°è™•ç†ç¿»è­¯ä»»å‹™ã€‚
```

é€²éšä½¿ç”¨æ™‚ï¼Œæ‚¨ä¹Ÿå¯ä»¥ç›´æ¥ä½¿ç”¨å·¥å…·ï¼š

```javascript
// å‘¼å«ç‰¹å®šé…ç½® (Qwen æ¨¡å‹)
const result = await mcp.callTool('llm-complete', {
  prompt: 'æ’°å¯«è™•ç† CSV è³‡æ–™çš„ Python å‡½æ•¸',
  configName: 'local',  // ä½¿ç”¨ Qwen é…ç½®
  maxTokens: 1000
});

// åˆ—å‡ºå¯ç”¨é…ç½®
const configs = await mcp.callTool('llm-list-configs', {});
```

## å·¥å…·æ–¹æ³•

### `llm-complete`

ç™¼é€æç¤ºè©åˆ° LLM ä¸¦å–å¾—å›æ‡‰ã€‚

**åƒæ•¸ï¼š**
- `prompt` (string): è¦ç™¼é€çš„æç¤ºè©
- `configName` (string, optional): é…ç½®åç¨±ï¼Œé è¨­ç‚º 'default'
- `temperature` (number, optional): æº«åº¦åƒæ•¸ (0.0 åˆ° 2.0)
- `maxTokens` (number, optional): æœ€å¤§ token æ•¸

**å›å‚³ï¼š**
```typescript
{
  content: [{
    type: 'text',
    text: string      // LLM å›æ‡‰å…§å®¹
  }],
  isError: boolean    // æ˜¯å¦ç™¼ç”ŸéŒ¯èª¤
}
```

### `llm-list-configs`

åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„ LLM é…ç½®ã€‚

**å›å‚³ï¼š**
```typescript
{
  content: [{
    type: 'text',
    text: string      // JSON æ ¼å¼çš„é…ç½®åˆ—è¡¨
  }],
  isError: boolean    // æ˜¯å¦ç™¼ç”ŸéŒ¯èª¤
}
```

## é–‹ç™¼

```bash
# é–‹ç™¼æ¨¡å¼ (ç›£è½æª”æ¡ˆè®ŠåŒ–)
npm run dev

# ç·¨è­¯ TypeScript
npm run build

# åŸ·è¡Œç·¨è­¯å¾Œçš„ç¨‹å¼
npm run start
```

## æˆæ¬Š

MIT License